{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"2023-DGM-MIPT-course\"\n",
    "!if [ -d {REPO_NAME} ]; then rm -Rf {REPO_NAME}; fi\n",
    "!git clone https://github.com/r-isachenko/{REPO_NAME}.git\n",
    "!cd {REPO_NAME}\n",
    "!pip install ./{REPO_NAME}/homeworks/\n",
    "!rm -Rf {REPO_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgm_utils import train_model, show_samples, plot_training_curves\n",
    "from dgm_utils import visualize_images, load_dataset\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "\n",
    "import wandb\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "print(\"cuda is available:\", USE_CUDA)\n",
    "if USE_CUDA:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils funcations\n",
    "\n",
    "def plot_losses(losses: np.ndarray, title: str):\n",
    "    n_itr = len(losses)\n",
    "    xs = np.arange(n_itr)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(xs, losses)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Iterations\", fontsize=14)\n",
    "    plt.ylabel(\"Loss\", fontsize=14)\n",
    "\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "visualize_images(train_data, \"MNIST samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_embeddings: int = 128, embedding_dim: int = 16, beta: float = 0.25, levels = [1, 2, 4, 7]) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "        \n",
    "        self.mlp_for_phis = nn.ModuleList()\n",
    "        for i in range(len(levels)):\n",
    "            mlp = []\n",
    "            mlp.append(nn.Conv2d(embedding_dim, embedding_dim, kernel_size=1))\n",
    "            mlp.append(nn.SiLU())\n",
    "            mlp.append(nn.Conv2d(embedding_dim, embedding_dim, kernel_size=1))\n",
    "            self.mlp_for_phis.append(nn.Sequential(*mlp))\n",
    "        \n",
    "\n",
    "    def get_code_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = x.shape[:-1]\n",
    "        flattened = x.view(-1, self.embedding_dim)\n",
    "        \n",
    "        # calculate distances from flatten inputs to embeddings\n",
    "        # find nearest embeddings to each input (use argmin op)\n",
    "\n",
    "        distances = (torch.sum(flattened ** 2, dim=1, keepdim=True) +\n",
    "                     torch.sum(self.embedding.weight ** 2, dim=1) -\n",
    "                     2 * torch.matmul(flattened, self.embedding.weight.t())\n",
    "                     )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        \n",
    "        encoding_indices = encoding_indices.view(input_shape)\n",
    "        return encoding_indices\n",
    "\n",
    "    def get_quantized(self, encoding_indices: torch.Tensor) -> torch.Tensor:\n",
    "        # get embeddgins with appropriate indices\n",
    "        # transform tensor from BHWC to BCHW format\n",
    "        quantized = self.embedding(encoding_indices).permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        return quantized\n",
    "        \n",
    "    def phis(self, zq: torch.Tensor, stage_ratio: float) -> torch.Tensor:\n",
    "        K = len(self.mlp_for_phis)\n",
    "        x = stage_ratio * (K - 1)\n",
    "        i_0 = int(torch.floor(torch.tensor(x)).item())\n",
    "        i_1 = min(K - 1, i_0 + 1)\n",
    "        delta = x - i_0\n",
    "\n",
    "        phi_i_0 = self.mlp_for_phis[i_0](zq)\n",
    "        if i_0 == i_1:\n",
    "            return phi_i_0\n",
    "        else:\n",
    "            phi_i_1 = self.mlp_for_phis[i_1](zq)\n",
    "            return (1 - delta) * phi_i_0 + delta * phi_i_1\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        \n",
    "        # get indices -> get quantized latents -> calculate codebook and commitment loss\n",
    "        # final loss is codebook_loss + beta * commitment_loss\n",
    "\n",
    "        quantized = self.get_quantized(self.get_code_indices(x))\n",
    "\n",
    "        loss = torch.mean((quantized.detach() - x)**2) + self.beta * torch.mean((quantized - x.detach())**2)\n",
    "\n",
    "        # Straight-through estimator!!! \n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        return quantized, loss\n",
    "\n",
    "\n",
    "def test_vector_quantizer():\n",
    "    x = torch.zeros((1, 16, 7, 7))\n",
    "    layer = VectorQuantizer()\n",
    "    indices = layer.get_code_indices(x)\n",
    "    assert indices.shape == (1, 7, 7)\n",
    "    quantized = layer.get_quantized(indices)\n",
    "    assert quantized.shape == (1, 16, 7, 7)\n",
    "    quantized, loss = layer(x)\n",
    "    assert quantized.shape == (1, 16, 7, 7)\n",
    "    assert loss.shape == ()\n",
    "\n",
    "\n",
    "test_vector_quantizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        encoder = []\n",
    "        encoder.append(nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1))\n",
    "        encoder.append(nn.ReLU(inplace=True))\n",
    "        encoder.append(nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1))\n",
    "        encoder.append(nn.ReLU(inplace=True))\n",
    "        encoder.append(nn.Conv2d(64, latent_dim, kernel_size=3, stride=1, padding=1))\n",
    "        encoder.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.net = nn.Sequential(*encoder)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        decoder = []\n",
    "        decoder.append(nn.ConvTranspose2d(latent_dim, 64, kernel_size=3, stride=1, padding=1))\n",
    "        decoder.append(nn.ReLU(inplace=True))\n",
    "        decoder.append(nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1))\n",
    "        decoder.append(nn.ReLU(inplace=True))\n",
    "        decoder.append(nn.ConvTranspose2d(32, 2, kernel_size=4, stride=2, padding=1))\n",
    "\n",
    "\n",
    "        self.net = nn.Sequential(*decoder)\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAEModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ce_loss_scale: float = 1.0,\n",
    "        latent_dim: int = 16,\n",
    "        num_embeddings: int = 64,\n",
    "        latent_size: tuple = (7, 7),\n",
    "        levels: list[int] = [1, 2, 4, 7],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(latent_dim)\n",
    "        self.decoder = ConvDecoder(latent_dim)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, latent_dim, beta = 0.25, levels=levels)\n",
    "        self.ce_loss_scale = ce_loss_scale\n",
    "        self.latent_size = latent_size\n",
    "        self.levels = levels\n",
    "\n",
    "\n",
    "    def multi_scale_forward(self, x: torch.Tensor) -> tuple:\n",
    "        B = x.size(0)\n",
    "        C = self.vq_layer.embedding_dim\n",
    "\n",
    "        z = self.encoder(x)\n",
    "        q, vq_loss = self.vq_layer(z)\n",
    "        idx7 = self.vq_layer.get_code_indices(z)\n",
    "        \n",
    "        latent_H, latent_W = self.latent_size\n",
    "        canvas = torch.zeros(B, C, latent_H, latent_W, device=device)\n",
    "        \n",
    "        logits = {}\n",
    "        for k, p in enumerate(self.levels):\n",
    "            if p == self.latent_size[0]:\n",
    "                idx_p = idx7\n",
    "            else:\n",
    "                idx_p = F.interpolate(idx7.unsqueeze(1).float(), size=(p, p), mode=\"nearest\").long().squeeze(1)\n",
    "            zq_p = self.vq_layer.get_quantized(idx_p)\n",
    "\n",
    "            alpha = k / (len(self.levels) - 1)\n",
    "            h_p = self.vq_layer.phis(zq_p, alpha)\n",
    "            h_p_up = F.interpolate(h_p, size=(latent_H, latent_W), mode=\"bicubic\")\n",
    "            canvas = canvas + h_p_up\n",
    "\n",
    "            logits[p] = self.decoder(canvas)\n",
    "            \n",
    "        return logits, vq_loss\n",
    "\n",
    "    def multi_scale_loss(self, x: torch.Tensor) -> dict:\n",
    "\n",
    "        target = x.squeeze(1).long()\n",
    "        logits, vq_loss = self.multi_scale_forward(x)\n",
    "        \n",
    "        z = self.encoder(x)\n",
    "        q, _ = self.vq_layer(z)\n",
    "        logits_raw = self.decoder(q) \n",
    "        \n",
    "        rec_loss = 0.0\n",
    "        \n",
    "        # w0 = 0.1\n",
    "        # rec_loss = rec_loss + w0 * F.cross_entropy(logits_raw, target)\n",
    "        # weights = {1: 0.1, 2: 0.2, 4: 0.3, 7: 0.4}\n",
    "\n",
    "        w_raw = 0.25\n",
    "        rec_loss = w_raw * F.cross_entropy(logits_raw, target)\n",
    "        weights = {1: 0.05, 2: 0.15, 4: 0.30, 7: 0.50}\n",
    "                 \n",
    "        for p, w in weights.items():\n",
    "            rec_loss = rec_loss + w * F.cross_entropy(logits[p], target)\n",
    "\n",
    "        total = self.ce_loss_scale * rec_loss + vq_loss\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": total,\n",
    "            \"recon_loss\": rec_loss,\n",
    "            \"vq_loss\": vq_loss,\n",
    "        }\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        \n",
    "        # apply encoder -> apply vector quantizer (it returns quantized representation + vq_loss) ->\n",
    "        # -> apply decoder (it returns decoded samples) \n",
    "\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        q, vq_loss = self.vq_layer(z)\n",
    "\n",
    "        decoded = self.decoder(q)\n",
    "\n",
    "        return decoded, vq_loss\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "    \n",
    "        # apply model -> get cross entropy loss\n",
    "\n",
    "        decoded, vq_loss = self.forward(x)\n",
    "        ce_loss = F.cross_entropy(decoded, x.squeeze(1).long())\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": self.ce_loss_scale * ce_loss + vq_loss,\n",
    "            \"ce_loss\": self.ce_loss_scale * ce_loss,\n",
    "            \"vq_loss\": vq_loss,\n",
    "        }\n",
    "\n",
    "    def get_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # apply encoder -> get indices of codes using vector quantizer\n",
    "        \n",
    "        z = self.encoder(x)\n",
    "        codebook_indices = self.vq_layer.get_code_indices(z)\n",
    "        return codebook_indices\n",
    "\n",
    "    def prior(self, n: int) -> torch.Tensor:\n",
    "        # get samples from categorical distribution -> get quantized representations using vector quantizer\n",
    "    \n",
    "        indices = torch.randint(0, self.vq_layer.num_embeddings, (n, *self.latent_size), device=\"cuda:0\")\n",
    "        quantized = self.vq_layer.get_quantized(indices)\n",
    "        return quantized\n",
    "\n",
    "    def sample_from_logits(self, logits: torch.Tensor) -> np.ndarray:\n",
    "        \n",
    "        # apply softmax to the logits -> sample from the distribution\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs = probs.permute(0, 2, 3, 1).contiguous()\n",
    "        B, H, W, C = probs.shape\n",
    "        probs_flat = probs.view(-1, C)\n",
    "        samples_flat = torch.multinomial(probs_flat, num_samples=1)\n",
    "        samples = samples_flat.view(B, H, W)\n",
    "        samples = samples.unsqueeze(1)\n",
    "\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # sample from prior distribution -> apply decoder -> sample from logits\n",
    "\n",
    "            quantized = self.prior(n)\n",
    "            logits = self.decoder(quantized)\n",
    "            samples = self.sample_from_logits(logits)\n",
    "            return samples\n",
    "\n",
    "\n",
    "def test_vqvae_model():\n",
    "    model = VQVAEModel().cuda()\n",
    "    x = torch.zeros((2, 1, 28, 28)).cuda()\n",
    "\n",
    "    encoded = model.encoder(x)\n",
    "    size = encoded.shape[2:]\n",
    "    assert size == model.latent_size\n",
    "\n",
    "    indices = model.get_indices(x)\n",
    "    assert indices.shape == (2, 7, 7)\n",
    "\n",
    "    losses = model.loss(x)\n",
    "    assert isinstance(losses, dict)\n",
    "    assert \"total_loss\" in losses\n",
    "\n",
    "    quantized = model.prior(10)\n",
    "    assert quantized.shape == (10, 16, *model.latent_size)\n",
    "\n",
    "    decoded = model.decoder(quantized)\n",
    "    assert decoded.shape == (10, 2, 28, 28)\n",
    "\n",
    "    sampled = model.sample(10)\n",
    "    assert sampled.shape == (10, 1, 28, 28)\n",
    "\n",
    "\n",
    "test_vqvae_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: object,\n",
    "    train_loader: object,\n",
    "    optimizer: object,\n",
    "    use_cuda: bool,\n",
    "    loss_key: str = \"total\",\n",
    ") -> defaultdict:\n",
    "    model.train()\n",
    "\n",
    "    stats = defaultdict(list)\n",
    "    for x in train_loader:\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        losses = model.multi_scale_loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        losses[loss_key].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            stats[k].append(v.item())\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def eval_model(model: object, data_loader: object, use_cuda: bool) -> defaultdict:\n",
    "    model.eval()\n",
    "    stats = defaultdict(float)\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "            losses = model.multi_scale_loss(x)\n",
    "            for k, v in losses.items():\n",
    "                stats[k] += v.item() * x.shape[0]\n",
    "\n",
    "        for k in stats.keys():\n",
    "            stats[k] /= len(data_loader.dataset)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: object,\n",
    "    train_loader: object,\n",
    "    test_loader: object,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    use_tqdm: bool = False,\n",
    "    use_cuda: bool = False,\n",
    "    loss_key: str = \"total_loss\",\n",
    ") -> Tuple[dict, dict]:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = defaultdict(list)\n",
    "    test_losses = defaultdict(list)\n",
    "    forrange = tqdm(range(epochs)) if use_tqdm else range(epochs)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    for epoch in forrange:\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, use_cuda, loss_key)\n",
    "        test_loss = eval_model(model, test_loader, use_cuda)\n",
    "\n",
    "        train_loss_epoch = train_loss[\"total_loss\"][-1]    # a single float\n",
    "        test_loss_epoch  = test_loss[\"total_loss\"]         # already a float\n",
    "        \n",
    "        wandb.log({\n",
    "            \"loss/train_total\": train_loss_epoch,\n",
    "            \"loss/train_recon\": train_loss[\"recon_loss\"][-1],\n",
    "            \"loss/train_vq\":    train_loss[\"vq_loss\"][-1],\n",
    "            \"loss/test_total\":  test_loss[\"total_loss\"],\n",
    "            \"loss/test_recon\":  test_loss[\"recon_loss\"],\n",
    "            \"loss/test_vq\":     test_loss[\"vq_loss\"],\n",
    "        }, step=epoch)\n",
    "\n",
    "        for k in train_loss.keys():\n",
    "            train_losses[k].extend(train_loss[k])\n",
    "            test_losses[k].append(test_loss[k])\n",
    "            \n",
    "        # wandb.log({\"loss_train_total_loss\": dict(train_losses)[\"total_loss\"], \"epoch\": epoch})\n",
    "        # wandb.log({\"loss_train_recon_loss\": dict(train_losses)[\"recon_loss\"], \"epoch\": epoch})\n",
    "        # wandb.log({\"loss_train_vq_loss\": dict(train_losses)[\"vq_loss\"], \"epoch\": epoch})\n",
    "\n",
    "        # wandb.log({\"loss_test_total_loss\": dict(test_losses)[\"total_loss\"], \"epoch\": epoch})\n",
    "        # wandb.log({\"loss_test_recon_loss\": dict(test_losses)[\"recon_loss\"], \"epoch\": epoch})\n",
    "        # wandb.log({\"loss_test_vq_loss\": dict(test_losses)[\"vq_loss\"], \"epoch\": epoch})\n",
    "        \n",
    "    return dict(train_losses), dict(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024   # any adequate value\n",
    "EPOCHS = 100       # < 30\n",
    "LR = 3e-4           # < 1e-2\n",
    "CE_SCALE = 5.0     # 0.01 < x < 30.0\n",
    "\n",
    "wandb.init(\n",
    "    project=\"VAR\",  \n",
    "    name=\"VQ-VAE_training\",\n",
    "    entity=\"andrew_tep\",   \n",
    "    config={\n",
    "        \"learning_rate\": LR,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "    }\n",
    ")\n",
    "\n",
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "\n",
    "model = VQVAEModel(ce_loss_scale=CE_SCALE, latent_dim=16, num_embeddings=128)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_losses, test_losses = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    use_cuda=USE_CUDA,\n",
    "    use_tqdm=True,\n",
    "    lr=LR,\n",
    ")\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in test_losses.items():\n",
    "    print(\"{}: {:.4f}\".format(key, value[-1]))\n",
    "\n",
    "samples = model.sample(100)\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"Samples\")\n",
    "\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    decoded, _ = model(x)\n",
    "    x_recon = model.sample_from_logits(decoded)\n",
    "x = x.cpu().numpy()\n",
    "reconstructions = np.concatenate((x, x_recon), axis=0)\n",
    "reconstructions = reconstructions.astype(\"float32\")\n",
    "show_samples(reconstructions, title=\"Reconstructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self, mask_type: str, in_channels: int, out_channels: int, kernel_size: int = 5\n",
    "    ) -> None:\n",
    "        assert mask_type in [\"A\", \"B\"]\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.register_buffer(\"mask\", torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.conv2d(input, self.weight * self.mask, self.bias, padding=self.padding)\n",
    "\n",
    "    def create_mask(self, mask_type: str) -> None:\n",
    "        k = self.kernel_size[0]\n",
    "        self.mask[:, :, : k // 2] = 1\n",
    "        self.mask[:, :, k // 2, : k // 2] = 1\n",
    "        if mask_type == \"B\":\n",
    "            self.mask[:, :, k // 2, k // 2] = 1\n",
    "\n",
    "\n",
    "def test_masked_conv2d():\n",
    "    layer = MaskedConv2d(\"A\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.zeros((2, 2)))\n",
    "\n",
    "    layer = MaskedConv2d(\"B\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.ones((2, 2)))\n",
    "\n",
    "\n",
    "test_masked_conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN_NX(nn.Module):\n",
    "    def __init__(\n",
    "        self, # нет vocab\n",
    "        num_embeddings: int = 128,\n",
    "        input_shape: tuple = (7,7), # current lavel size [1x1 -> 2x2 -> 4x4 -> 7x7]\n",
    "        n_filters: int = 32,\n",
    "        kernel_size: int = 5,\n",
    "        n_layers: int = 5,\n",
    "        num_stages: int = 4,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_embeddings = num_embeddings\n",
    "        \n",
    "        # self.stage_embed = nn.Embedding(num_stages, n_filters)\n",
    "        # self.in_proj = nn.Conv2d(num_embeddings, n_filters, 1)\n",
    "        \n",
    "        C_embed = 64                   \n",
    "        self.token_embed = nn.Embedding(num_embeddings, C_embed)\n",
    "        self.stage_embed = nn.Embedding(num_stages, n_filters)\n",
    "        self.in_proj = nn.Conv2d(C_embed, n_filters, 1)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(MaskedConv2d(\"A\", n_filters, n_filters, kernel_size=kernel_size)) # num_embeddings!=n_filters\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(MaskedConv2d(\"B\", n_filters, n_filters, kernel_size=kernel_size))\n",
    "\n",
    "        layers.extend(\n",
    "            [\n",
    "                nn.ReLU(),\n",
    "                MaskedConv2d(\"B\", in_channels=n_filters, out_channels=num_embeddings, kernel_size=1),\n",
    "            ]\n",
    "        )\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, prev_x_b1hw, stage_id: int, out_shape: Tuple[int, int]):\n",
    "        h,w = out_shape\n",
    "        x_upscale_b1hw = F.interpolate(prev_x_b1hw.float(), (h,w), mode=\"nearest\").long()\n",
    "\n",
    "        # flattened = x_upscale_b1hw.view((-1, 1))\n",
    "        # encodings = torch.zeros(flattened.shape[0], self.num_embeddings).cuda()\n",
    "        # encodings.scatter_(1, flattened, 1)\n",
    "        # encodings = encodings.view((-1, h, w, self.num_embeddings))\n",
    "        # encodings = encodings.permute((0, 3, 1, 2))\n",
    "\n",
    "        encodings = self.token_embed(x_upscale_b1hw.squeeze(1))        # B×H×W×C_e\n",
    "        encodings = encodings.permute(0,3,1,2).contiguous()  \n",
    "\n",
    "        #stage embedding\n",
    "        b = encodings.size(0)\n",
    "        # emb = self.stage_embed(torch.tensor([stage_id], device=encodings.device)).view(1, -1, 1, 1)\n",
    "        stage_id_tensor = torch.full((b,), stage_id, device=encodings.device, dtype=torch.long)\n",
    "        emb = self.stage_embed(stage_id_tensor).view(b, -1, 1, 1)\n",
    "        encodings = self.in_proj(encodings) + emb\n",
    "        \n",
    "        out = self.net(encodings)\n",
    "        out = out.view(-1, self.num_embeddings, 1, h, w)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (7, 7)\n",
    "\n",
    "\n",
    "train_indices_list = []\n",
    "test_indices_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in train_loader:\n",
    "        x = x.cuda()\n",
    "        indices = model.get_indices(x)\n",
    "        indices = indices.unsqueeze(1)\n",
    "        train_indices_list.append(indices.cpu().numpy())\n",
    "\n",
    "train_indices = np.concatenate(train_indices_list, axis=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in test_loader:\n",
    "        x = x.cuda()\n",
    "        indices = model.get_indices(x)\n",
    "        indices = indices.unsqueeze(1)\n",
    "        test_indices_list.append(indices.cpu().numpy())\n",
    "\n",
    "test_indices = np.concatenate(test_indices_list, axis=0)\n",
    "\n",
    "\n",
    "assert isinstance(train_indices, np.ndarray)\n",
    "assert isinstance(test_indices, np.ndarray)\n",
    "assert train_indices.shape == (60000, 1, *INPUT_SHAPE)\n",
    "assert test_indices.shape == (10000, 1, *INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(idx_7x7: torch.LongTensor, size: int) -> torch.LongTensor:\n",
    "    x = idx_7x7.unsqueeze(0).float()     # (1,1,7,7)\n",
    "    down = F.interpolate(x, size=(size, size), mode='nearest')\n",
    "    return down.long().squeeze(0)  \n",
    "    \n",
    "class MultiScaleIndices(Dataset):\n",
    "    def __init__(self, indices_7x7_np: np.ndarray):\n",
    "        self.idx7 = torch.from_numpy(indices_7x7_np).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx7)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx7 = self.idx7[i]               # 1×7×7\n",
    "        # [1x1 -> 2x2 -> 4x4 -> 7x7]\n",
    "        idx4 = downsample(idx7, size=4)   # 1×4×4\n",
    "        idx2 = downsample(idx7, size=2)   # 1×2×2\n",
    "        idx1 = downsample(idx7, size=1)   # 1×1×1\n",
    "        return {\n",
    "            1: idx1,  # 1×1×1\n",
    "            2: idx2,  # 1×2×2\n",
    "            4: idx4,  # 1×4×4\n",
    "            7: idx7,  # 1×7×7\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [1, 2, 4, 7]\n",
    "stage_ids = {1: 0, 2: 1, 4: 2, 7: 3}\n",
    "\n",
    "def train_epoch(\n",
    "    model: object,\n",
    "    train_loader: object,\n",
    "    optimizer: object,\n",
    "    use_cuda: bool,\n",
    "    loss_key: str = \"total\",\n",
    ") -> defaultdict:\n",
    "    model.train()\n",
    "\n",
    "    stats = defaultdict(list)\n",
    "    for batch in train_loader:\n",
    "        if use_cuda:\n",
    "            batch = {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "        for i in range(len(levels)-1):\n",
    "            prev = batch[levels[i]]\n",
    "            curr = batch[levels[i+1]]\n",
    "            stage_id = stage_ids[levels[i+1]]\n",
    "            \n",
    "            logits = model(prev, stage_id, out_shape=(levels[i+1], levels[i+1]))\n",
    "            loss = F.cross_entropy(logits.squeeze(2).squeeze(1), curr.squeeze(1).long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            stats[f\"loss_{levels[i+1]}\"].append(loss.item())\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def eval_model(model: object, data_loader: object, use_cuda: bool) -> defaultdict:\n",
    "    model.eval()\n",
    "    stats = defaultdict(float)\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            if use_cuda:\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            bs = next(iter(batch.values())).size(0)\n",
    "            total += bs\n",
    "            for i in range(len(levels)-1):\n",
    "                prev = batch[levels[i]]\n",
    "                curr = batch[levels[i+1]]\n",
    "                stage_id = stage_ids[levels[i+1]]\n",
    "                logits = model(prev, stage_id, out_shape=(levels[i+1], levels[i+1]))\n",
    "                # loss = F.cross_entropy(logits.squeeze(2).squeeze(1), curr.squeeze(1).long(), reduction=\"sum\")\n",
    "                # stats[f\"loss_{levels[i+1]}\"] += loss.item()\n",
    "                loss = F.cross_entropy(logits.squeeze(2).squeeze(1), curr.squeeze(1).long())\n",
    "                stats[f\"loss_{levels[i+1]}\"] += loss.item() * bs\n",
    "                \n",
    "        for k in stats:\n",
    "            stats[k] /= total\n",
    "    return stats\n",
    "\n",
    "\n",
    "def sample_nsp(model: PixelCNN_NX,\n",
    "               levels: list[int] = [1, 2, 4, 7],\n",
    "               stage_ids: dict = {1: 0, 2: 1, 4: 2, 7: 3},\n",
    "               num_samples: int = 8,\n",
    "               cfg_scale: float = 6.5,\n",
    "               device=\"cuda\",) -> torch.LongTensor:\n",
    "    samples = {levels[0]: torch.randint(\n",
    "        model.num_embeddings, \n",
    "        size=(num_samples, 1, levels[0], levels[0]),\n",
    "        device=device,\n",
    "        dtype=torch.long\n",
    ")}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(1, len(levels)):\n",
    "            prev_level = levels[i - 1]\n",
    "            curr_level = levels[i]\n",
    "            stage_id = stage_ids[curr_level]\n",
    "            out_shape = (curr_level, curr_level)\n",
    "    \n",
    "            curr_sample = torch.zeros(num_samples, 1, *out_shape, dtype=torch.long, device=device)\n",
    "    \n",
    "            for r in range(curr_level):\n",
    "                for c in range(curr_level):\n",
    "                    # logits=model(samples[prev_level], stage_id, out_shape).squeeze(2)\n",
    "                    inp = samples[prev_level].repeat(2,1,1,1)\n",
    "                    logits2 = model(inp, stage_id, out_shape).squeeze(2)\n",
    "                    logits_c, logits_u = logits2.chunk(2, 0)\n",
    "                    logits = logits_u + cfg_scale*(logits_c - logits_u)\n",
    "                    \n",
    "                    probs = F.softmax(logits[:, :, r, c], dim=1)\n",
    "                    token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                    curr_sample[:, 0, r, c] = token\n",
    "            samples[curr_level] = curr_sample\n",
    "        \n",
    "    return samples, samples[levels[-1]]\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_nsp(model_nsp, vq_vae, levels, stage_ids, num_samples=8, device=\"cuda\"):\n",
    "    model_nsp.eval()\n",
    "    \n",
    "    samples_dict = {levels[0]: torch.randint(\n",
    "        model_nsp.num_embeddings, \n",
    "        size=(num_samples, 1, levels[0], levels[0]),\n",
    "        device=device,\n",
    "        dtype=torch.long\n",
    "    )}\n",
    "    \n",
    "    for i in range(1, len(levels)):\n",
    "        prev_level = levels[i - 1]\n",
    "        curr_level = levels[i]\n",
    "        stage_id = stage_ids[curr_level]\n",
    "        out_shape = (curr_level, curr_level)\n",
    "\n",
    "        curr_sample = torch.zeros(num_samples, 1, *out_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        for r in range(curr_level):\n",
    "            for c in range(curr_level):\n",
    "                logits = model_nsp(samples_dict[prev_level], stage_id, out_shape).squeeze(2)\n",
    "                probs = F.softmax(logits[:, :, r, c], dim=1)\n",
    "                token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                curr_sample[:, 0, r, c] = token\n",
    "        samples_dict[curr_level] = curr_sample\n",
    "\n",
    "    rep_imgs = []\n",
    "    latent_H, latent_W = model.latent_size\n",
    "    for p in levels:\n",
    "        idx = samples_dict[p].squeeze(1)\n",
    "        quantized = vq_vae.vq_layer.get_quantized(torch.Tensor(idx))\n",
    "\n",
    "        quantized = F.interpolate(\n",
    "            quantized,\n",
    "            size=(latent_H,latent_W),\n",
    "            mode=\"bicubic\"\n",
    "        )\n",
    "        logits = vq_vae.decoder(quantized)\n",
    "        samples = vq_vae.sample_from_logits(logits)\n",
    "        \n",
    "        samples = samples.astype(\"float32\")\n",
    "        rep_imgs.append(samples[0].squeeze(0))\n",
    "        # show_samples(samples, title=f\"NSP @ {p}×{p}\")\n",
    "        \n",
    "    fig, axes = plt.subplots(1, len(levels), \n",
    "                             figsize=(len(levels) * 3, 3))\n",
    "    for ax, img, p in zip(axes, rep_imgs, levels):\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"NSP @ {p}×{p}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_nsp: object,\n",
    "    vq_vae: object,\n",
    "    levels, \n",
    "    stage_ids,\n",
    "    train_loader: object,\n",
    "    test_loader: object,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    use_tqdm: bool = False,\n",
    "    use_cuda: bool = False,\n",
    "    loss_key: str = \"total_loss\",\n",
    ") -> Tuple[dict, dict]:\n",
    "    optimizer = optim.Adam(model_nsp.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = defaultdict(list)\n",
    "    test_losses = defaultdict(list)\n",
    "    forrange = tqdm(range(epochs)) if use_tqdm else range(epochs)\n",
    "    if use_cuda:\n",
    "        model_nsp = model_nsp.cuda()\n",
    "\n",
    "    for epoch in forrange:\n",
    "        model_nsp.train()\n",
    "        train_loss = train_epoch(model_nsp, train_loader, optimizer, use_cuda, loss_key)\n",
    "        test_loss = eval_model(model_nsp, test_loader, use_cuda)\n",
    "\n",
    "        # print(train_loss)\n",
    "        train_loss_epoch = train_loss[\"loss_7\"][-1]    \n",
    "        test_loss_epoch  = test_loss[\"loss_7\"]        \n",
    "        \n",
    "        wandb.log({\n",
    "            \"loss/train_7\": train_loss_epoch,\n",
    "            \"loss/train_4\": train_loss[\"loss_4\"][-1],\n",
    "            \"loss/train_2\":    train_loss[\"loss_2\"][-1],\n",
    "            \"loss/test_7\":  test_loss[\"loss_7\"],\n",
    "            \"loss/test_4\":  test_loss[\"loss_4\"],\n",
    "            \"loss/test_2\":     test_loss[\"loss_2\"],\n",
    "        }, step=epoch)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(\"EPOCH:\", epoch + 1)\n",
    "            visualize_nsp(model_nsp, vq_vae, levels, stage_ids)\n",
    "\n",
    "        for k in train_loss.keys():\n",
    "            train_losses[k].extend(train_loss[k])\n",
    "            test_losses[k].append(test_loss[k])\n",
    "    return dict(train_losses), dict(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100 \n",
    "BATCH_SIZE = 1024\n",
    "LR = 2e-5     \n",
    "N_LAYERS = 30   \n",
    "N_FILTERS = 128\n",
    "\n",
    "\n",
    "prior_model = PixelCNN_NX(num_embeddings=128, \n",
    "                          input_shape=INPUT_SHAPE, \n",
    "                          n_filters=N_FILTERS, \n",
    "                          kernel_size=5, \n",
    "                          n_layers=N_LAYERS\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project=\"VAR\",  \n",
    "    name=\"PixelCNN_training\",\n",
    "    entity=\"andrew_tep\",   \n",
    "    config={\n",
    "        \"learning_rate\": LR,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(MultiScaleIndices(train_indices), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(MultiScaleIndices(test_indices), batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(\n",
    "    prior_model,\n",
    "    model,\n",
    "    levels,\n",
    "    stage_ids,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    use_tqdm=True,\n",
    "    use_cuda=USE_CUDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_coarse_to_fine_samples(\n",
    "    model_nsp,\n",
    "    vq_vae,\n",
    "    levels: list[int],\n",
    "    stage_ids: dict[int,int],\n",
    "    num_samples: int = 4,\n",
    "    device: str = \"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Для num_samples случайных 1×1 токенов визуализирует прогресс генерации:\n",
    "      raw grid → decoded image\n",
    "    на каждом уровне levels (например [1,2,4,7]).\n",
    "    \"\"\"\n",
    "    model_nsp.eval()\n",
    "    device = torch.device(device)\n",
    "\n",
    "    \n",
    "    samples_idx = {\n",
    "        levels[0]: torch.randint(\n",
    "            0,\n",
    "            model_nsp.num_embeddings,\n",
    "            (num_samples, 1, levels[0], levels[0]),\n",
    "            device=device,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "    }\n",
    "    for i in range(1, len(levels)):\n",
    "        prev_p = levels[i-1]\n",
    "        p      = levels[i]\n",
    "        stage  = stage_ids[p]\n",
    "        out_hw = (p, p)\n",
    "\n",
    "    \n",
    "        logits = model_nsp(\n",
    "            samples_idx[prev_p],\n",
    "            stage_id=stage,\n",
    "            out_shape=out_hw\n",
    "        ).squeeze(2)  # (B, V, p, p)\n",
    "\n",
    "\n",
    "        curr = torch.zeros((num_samples,1,p,p), device=device, dtype=torch.long)\n",
    "        for r in range(p):\n",
    "            for c in range(p):\n",
    "                probs    = F.softmax(logits[:,:,r,c], dim=1)      # (B, V)\n",
    "                curr[:,0,r,c] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        samples_idx[p] = curr\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        num_samples,\n",
    "        len(levels)*2,\n",
    "        figsize=(2*len(levels), 2*num_samples),\n",
    "        squeeze=False\n",
    "    )\n",
    "\n",
    "    latent_H, latent_W = model.latent_size\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for l_idx, p in enumerate(levels):\n",
    "            \n",
    "            ax = axes[i][2*l_idx]\n",
    "            grid = samples_idx[p][i,0].cpu().numpy() \n",
    "            ax.imshow(grid, cmap=\"tab20\")\n",
    "            ax.set_title(f\"{p}×{p}\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "\n",
    "            idx = samples_idx[p][i:i+1,0]            \n",
    "            zq = vq_vae.vq_layer.get_quantized(torch.Tensor(idx).int().cuda())              \n",
    "\n",
    "        \n",
    "            zq_up       = F.interpolate(zq, size=(latent_H,latent_W), mode=\"bicubic\")\n",
    "            recon_logits = vq_vae.decoder(zq_up)      \n",
    "            img         = vq_vae.sample_from_logits(recon_logits)[0,0] \n",
    "\n",
    "            ax = axes[i][2*l_idx+1]\n",
    "            ax.imshow(img, cmap=\"gray\")\n",
    "            ax.set_title(\"decoded\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_coarse_to_fine_samples(\n",
    "    model_nsp=prior_model,\n",
    "    vq_vae=model,\n",
    "    levels=levels,\n",
    "    stage_ids=stage_ids,\n",
    "    num_samples=6,    \n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(train_losses[\"loss_7\"]), min(test_losses[\"loss_7\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "H, W = 7, 7\n",
    "bpd = np.mean(train_losses[\"loss_7\"]) / (math.log(2) * H * W)\n",
    "print(f\"Bits-per-dim @7×7: {bpd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sampled_indices = sample_nsp(prior_model, num_samples=100, device=\"cuda\")[1].squeeze(1)\n",
    "\n",
    "quantized = model.vq_layer.get_quantized(torch.Tensor(sampled_indices).int().cuda())\n",
    "logits = model.decoder(quantized)\n",
    "samples = model.sample_from_logits(logits)\n",
    "\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "samples_dict = sample_nsp(prior_model, num_samples=100, device=\"cuda\")[0]\n",
    "\n",
    "B = samples_dict[1].shape[0]\n",
    "C = model.vq_layer.embedding_dim\n",
    "latent_H, latent_W = model.latent_size \n",
    "\n",
    "canvas = torch.zeros(B, C, latent_H, latent_W, device=\"cuda\")\n",
    "\n",
    "for k, p in enumerate(levels):\n",
    "    idx = samples_dict[p][:,0]\n",
    "\n",
    "    zq = model.vq_layer.get_quantized(idx)\n",
    "\n",
    "    ratio = k/(len(levels)-1)\n",
    "    h_p   = model.vq_layer.phis(zq, ratio)\n",
    "\n",
    "    h_p_up = F.interpolate(h_p, size=(latent_H, latent_W), mode=\"bicubic\")\n",
    "    canvas  = canvas + h_p_up       \n",
    "\n",
    "recon_logits = model.decoder(canvas)  \n",
    "samples = model.sample_from_logits(recon_logits)\n",
    "\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"VAR-style Samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def add_level_to_canvas(idx_b1hw: torch.LongTensor,\n",
    "                        stage_ratio: float,\n",
    "                        vqvae: VQVAEModel,\n",
    "                        canvas_bChw: torch.Tensor,\n",
    "                        target_hw: tuple[int, int]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Добавляет вклад от текущего уровня в общий холст.\n",
    "    \"\"\"\n",
    "    zq = vqvae.vq_layer.get_quantized(idx_b1hw.squeeze(1))      # (B,C,h,w)\n",
    "    h  = vqvae.vq_layer.phis(zq, stage_ratio)                   # (B,C,h,w)\n",
    "    H, W = target_hw\n",
    "    h_up = F.interpolate(h, size=(H, W), mode=\"bicubic\")\n",
    "    return canvas_bChw + h_up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_var_style(model_nsp, vqvae,\n",
    "                     levels=[1,2,4,7],\n",
    "                     stage_ids={1:0,2:1,4:2,7:3},\n",
    "                     cfg_scale=3.0,\n",
    "                     num_samples=16,\n",
    "                     device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Coarse→Fine + φ-canvas + Classifier-Free Guidance.\n",
    "    Возвращает numpy-массив изображений (B,1,28,28) float32.\n",
    "    \"\"\"\n",
    "    device  = torch.device(device)\n",
    "    B       = num_samples\n",
    "    C       = vqvae.vq_layer.embedding_dim\n",
    "    H, W    = vqvae.latent_size\n",
    "\n",
    "    idx = {levels[0]: torch.randint(model_nsp.num_embeddings,\n",
    "                                    (B,1,1,1), device=device)}\n",
    "\n",
    "    canvas = torch.zeros(B, C, H, W, device=device)\n",
    "\n",
    "    for i in range(1, len(levels)):\n",
    "        p        = levels[i]\n",
    "        prev_p   = levels[i-1]\n",
    "        stage_id = stage_ids[p]\n",
    "\n",
    "        logits_cond   = model_nsp(idx[prev_p],        stage_id, (p,p)).squeeze(2)\n",
    "        logits_uncond = model_nsp(idx[prev_p]*0,      stage_id, (p,p)).squeeze(2)\n",
    "\n",
    "        logits = logits_uncond + cfg_scale*(logits_cond - logits_uncond)\n",
    "\n",
    "        idx_p = torch.zeros(B,1,p,p, device=device, dtype=torch.long)\n",
    "        for r in range(p):\n",
    "            for c in range(p):\n",
    "                probs            = F.softmax(logits[:,:,r,c], dim=1)\n",
    "                idx_p[:,0,r,c]   = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        idx[p] = idx_p\n",
    "\n",
    "        ratio   = i/(len(levels)-1)\n",
    "        canvas  = add_level_to_canvas(idx_p, ratio, vqvae, canvas, (H,W))\n",
    "\n",
    "    recon_logits = vqvae.decoder(canvas)\n",
    "    imgs         = vqvae.sample_from_logits(recon_logits)       # numpy (B,1,28,28)\n",
    "    return imgs.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "samples = sample_var_style(prior_model, model,\n",
    "                           levels=levels,\n",
    "                           stage_ids=stage_ids,\n",
    "                           cfg_scale=3.0,\n",
    "                           num_samples=64,\n",
    "                           device=\"cuda\")\n",
    "show_samples(samples, title=\"VAR + φ-canvas + CFG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
